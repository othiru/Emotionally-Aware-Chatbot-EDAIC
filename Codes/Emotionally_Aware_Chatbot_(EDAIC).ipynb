{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotionally Aware Chatbot (EDAIC).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Emotionally Aware Chatbot (EDAIC)\n",
        "Emotion is one of the basic instincts of a human being. Emotion detection plays a vital role in the field of textual analysis. At present, people’s expressions and emotional states have turned into the leading topic for research works. People train chatbot that is a software program with artificial intelligence for detecting emotions and many other purposes. In this paper, a chatbot is created to converse with humans to find their emotional states through machine learning techniques.\n",
        "\n",
        "EDAIC = Emotion Detection & Artificial Intellegent Chatbot\n",
        "\n",
        "**Course No:** CSE4250\n",
        "\n",
        "**Course Name:** Project & Thesis II\n",
        "\n",
        "**Supervisor**\n",
        "\n",
        "\n",
        "*   Md Khairul Hasan\n",
        "\n",
        "**Team Members**\n",
        "\n",
        "*   160204107 - Nowshin Rumali\n",
        "*   170104061 - Amin Ahmed Toshib\n",
        "*   170104116 - Rejone-E-Rasul Hridoy\n",
        "*   170104118 - Mehedi Hasan Sami\n"
      ],
      "metadata": {
        "id": "BxMo1vVG4VIa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUpYsfh1NUhi",
        "outputId": "96bf19ed-d489-4688-c7c3-2e1cc410fb89"
      },
      "source": [
        "# Basic Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Text Libraries\n",
        "import nltk \n",
        "import string\n",
        "import re\n",
        "\n",
        "# Feature Extraction Libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Classifier Model libraries\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import tree\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Performance Matrix libraries\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# other\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "from scipy.spatial import distance\n",
        "from datetime import datetime\n",
        "!pip install pendulum\n",
        "!pip install nameparser\n",
        "import pendulum\n",
        "from nameparser.parser import HumanName\n",
        "from nltk.corpus import wordnet\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = \"/content/drive/MyDrive/CSE/4.1/CSE4100 - Project & Thesis-I/Emotion Detection Chatbot Papers/4.2/\"\n",
        "resource_root_path = \"/content/drive/MyDrive/CSE/4.2/CSE4238 - Soft Computing Lab/Project - Emotion Detection from Twitter Text/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pendulum\n",
            "  Downloading pendulum-2.1.2-cp37-cp37m-manylinux1_x86_64.whl (155 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 155 kB 10.2 MB/s \n",
            "\u001b[?25hCollecting pytzdata>=2020.1\n",
            "  Downloading pytzdata-2020.1-py2.py3-none-any.whl (489 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 42.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 45.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 46.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 45.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 47.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 122 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 133 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 143 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 153 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 163 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 174 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 184 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 194 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 204 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 215 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 225 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 235 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 245 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 256 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 266 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 276 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 286 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 296 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 307 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 317 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 327 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 337 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 348 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 358 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 368 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 378 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 389 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 399 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 409 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 419 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 430 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 440 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 450 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 460 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 471 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 481 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 489 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.7/dist-packages (from pendulum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0,>=2.6->pendulum) (1.15.0)\n",
            "Installing collected packages: pytzdata, pendulum\n",
            "Successfully installed pendulum-2.1.2 pytzdata-2020.1\n",
            "Collecting nameparser\n",
            "  Downloading nameparser-1.0.6-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: nameparser\n",
            "Successfully installed nameparser-1.0.6\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnLshlp7NyCg"
      },
      "source": [
        "# 1. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G7_dcg7N0w6"
      },
      "source": [
        "### 1.1 Chatbot Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AYTUGnfRN5k5",
        "outputId": "73d1669d-63cc-4e28-adff-cf4aa76363cb"
      },
      "source": [
        "df_chatbot = pd.read_csv(root_path+'Chatbot Dataset_v12.11.csv',encoding='ISO-8859-1')\n",
        "df_chatbot = df_chatbot.dropna(axis=0)\n",
        "df_chatbot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Chatbot</th>\n",
              "      <th>Intent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello</td>\n",
              "      <td>Hi &lt;HUMAN&gt; how are you?</td>\n",
              "      <td>Greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi</td>\n",
              "      <td>Hello &lt;HUMAN&gt; how are you?</td>\n",
              "      <td>Greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hola</td>\n",
              "      <td>Hi &lt;HUMAN&gt; how are you?</td>\n",
              "      <td>Greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi there</td>\n",
              "      <td>Hi &lt;HUMAN&gt; how are you?</td>\n",
              "      <td>Greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hya there</td>\n",
              "      <td>Hi &lt;HUMAN&gt; how are you?</td>\n",
              "      <td>Greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2363</th>\n",
              "      <td>Today I meditated for 30 minutes and I feel am...</td>\n",
              "      <td>I am glad you felt better after meditating</td>\n",
              "      <td>Surprise_Amazed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2364</th>\n",
              "      <td>I broke my foot</td>\n",
              "      <td>I am sorry to hear your foot broke</td>\n",
              "      <td>Health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>I broke my foot</td>\n",
              "      <td>I am sorry to hear you broke your foot</td>\n",
              "      <td>Health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2366</th>\n",
              "      <td>My boss gave me priase in front of the group a...</td>\n",
              "      <td>I am glad your work was praised</td>\n",
              "      <td>Happy_Excited_Joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2367</th>\n",
              "      <td>My boss gave me priase in front of the group a...</td>\n",
              "      <td>I am happy to hear about the praise you got at...</td>\n",
              "      <td>Happy_Excited_Joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2064 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   User  ...             Intent\n",
              "1                                                 Hello  ...           Greeting\n",
              "2                                                    Hi  ...           Greeting\n",
              "3                                                  Hola  ...           Greeting\n",
              "4                                              Hi there  ...           Greeting\n",
              "5                                             Hya there  ...           Greeting\n",
              "...                                                 ...  ...                ...\n",
              "2363  Today I meditated for 30 minutes and I feel am...  ...    Surprise_Amazed\n",
              "2364                                    I broke my foot  ...             Health\n",
              "2365                                    I broke my foot  ...             Health\n",
              "2366  My boss gave me priase in front of the group a...  ...  Happy_Excited_Joy\n",
              "2367  My boss gave me priase in front of the group a...  ...  Happy_Excited_Joy\n",
              "\n",
              "[2064 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yqTSh14pNzf7",
        "outputId": "e3ab2778-fdc7-426d-bd3c-21572705d64d"
      },
      "source": [
        "df_emotion = pd.read_csv(root_path+'text_emotions_neutral.csv')\n",
        "df_emotion"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i didnt feel humiliated</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i can go from feeling so hopeless to so damned...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>Yeah.  Did you know that in Nevada there is a...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>I wonder why, not many have had facial hair a...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>That is sad, it is bad that we really wind up...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>Same here.  Since 1900 the taller candidate h...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>I do, I even read a book about their developm...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content sentiment\n",
              "0                                i didnt feel humiliated   sadness\n",
              "1      i can go from feeling so hopeless to so damned...   sadness\n",
              "2       im grabbing a minute to post i feel greedy wrong     anger\n",
              "3      i am ever feeling nostalgic about the fireplac...      love\n",
              "4                                   i am feeling grouchy     anger\n",
              "...                                                  ...       ...\n",
              "24995   Yeah.  Did you know that in Nevada there is a...   Neutral\n",
              "24996   I wonder why, not many have had facial hair a...   Neutral\n",
              "24997   That is sad, it is bad that we really wind up...   Neutral\n",
              "24998   Same here.  Since 1900 the taller candidate h...   Neutral\n",
              "24999   I do, I even read a book about their developm...   Neutral\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q_AV5W8O7Ni"
      },
      "source": [
        "# 2. Preporcessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX2OZdAmO6i5"
      },
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self,Remove_stopwords=True):\n",
        "        self.emojis = pd.read_csv(resource_root_path+'emojis.txt',sep=',',header=None)\n",
        "        self.emojis_dict = {i:j for i,j in zip(self.emojis[0],self.emojis[1])}\n",
        "        self.pattern = '|'.join(sorted(re.escape(k) for k in self.emojis_dict))\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        self.rmv_stopword = Remove_stopwords\n",
        "\n",
        "    def replace_emojis(self, text):\n",
        "        text = re.sub(self.pattern,lambda m: self.emojis_dict.get(m.group(0)), text, flags=re.IGNORECASE)\n",
        "        return text\n",
        "\n",
        "    def remove_punct(self, text):\n",
        "        text = self.replace_emojis(text)\n",
        "        text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "        text = re.sub('[0-9]+', '', text)\n",
        "        return text\n",
        "\n",
        "    def tokenization(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.split('\\W+', text)\n",
        "        return text\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        stopword = nltk.corpus.stopwords.words('english')\n",
        "        stopword.extend(['yr', 'year', 'woman', 'man', 'girl','boy','one', 'two', 'sixteen', 'yearold', 'fu', 'weeks', 'week',\n",
        "              'treatment', 'associated', 'patients', 'may','day', 'case','old','u','n','didnt','ive','ate','feel','keep'\n",
        "                ,'brother','dad','basic','im',''])\n",
        "        \n",
        "        text = [word for word in text if word not in stopword]\n",
        "        return text\n",
        "\n",
        "    def lemmatizer(self, text):\n",
        "        wn = nltk.WordNetLemmatizer()\n",
        "        text = [wn.lemmatize(word) for word in text]\n",
        "        return text\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = self.remove_punct(text)\n",
        "        text = self.tokenization(text)\n",
        "        if self.rmv_stopword == True:\n",
        "            text = self.remove_stopwords(text)\n",
        "        text = self.lemmatizer(text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L5on6JQR_w1",
        "outputId": "b7bc0d3e-c8f0-48f6-b6bd-13ff76639873"
      },
      "source": [
        "preprocess = Preprocessing(Remove_stopwords=False)\n",
        "\n",
        "df_test = df_chatbot['User'].apply(lambda x: preprocess.clean_text(x))\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1                                                [hello]\n",
              "2                                                   [hi]\n",
              "3                                                 [hola]\n",
              "4                                            [hi, there]\n",
              "5                                           [hya, there]\n",
              "                             ...                        \n",
              "621    [i, even, feel, surprised, if, it, dark, outside]\n",
              "622    [i, have, chose, something, for, myself, that,...\n",
              "623            [the, thunderstorm, really, surprise, me]\n",
              "624    [i, am, amazed, by, nature, and, amazed, by, l...\n",
              "625                      [wow, thats, really, amazing, ]\n",
              "Name: User, Length: 596, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0d00Y9cQ_at"
      },
      "source": [
        "# 3. Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jiy9VbnzTvVh"
      },
      "source": [
        "class FeatureExtraction:\n",
        "    def __init__(self,rmv_stopword=True):\n",
        "        self.rmv_stopword = rmv_stopword\n",
        "        self.preprocess = Preprocessing(self.rmv_stopword)\n",
        "        self.countVectorizer1 = CountVectorizer(analyzer=self.preprocess.clean_text)\n",
        "        self.tfidf_transformer_xtrain = TfidfTransformer()\n",
        "        self.tfidf_transformer_xtest = TfidfTransformer()\n",
        "\n",
        "    def get_features(self, X_train, X_test):\n",
        "        # countVectorizer1 = CountVectorizer(analyzer=self.preprocess.clean_text)\n",
        "        countVector1 = self.countVectorizer1.fit_transform(X_train)\n",
        "\n",
        "        countVector2 = self.countVectorizer1.transform(X_test)\n",
        "\n",
        "        # tfidf_transformer_xtrain = TfidfTransformer()\n",
        "        x_train = self.tfidf_transformer_xtrain.fit_transform(countVector1)\n",
        "\n",
        "        # tfidf_transformer_xtest = TfidfTransformer()\n",
        "        x_test = self.tfidf_transformer_xtest.fit_transform(countVector2)\n",
        "\n",
        "        return x_train, x_test\n",
        "\n",
        "    def get_processed_text(self, input_str):\n",
        "        return self.tfidf_transformer_xtest.fit_transform(self.countVectorizer1.transform([input_str]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQgL5OQNRECW"
      },
      "source": [
        "## 3.1 Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad3qGEn8Uvok",
        "outputId": "1f87d895-8be1-4540-8a76-3c3d9f0566eb"
      },
      "source": [
        "X_train_ed, X_test_ed, y_train_ed, y_test_ed = train_test_split(df_emotion['content'], df_emotion['sentiment'],test_size=0.3, random_state = 116)\n",
        "X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(df_chatbot['User'], df_chatbot['Intent'],test_size=0.25, random_state = 16)\n",
        "\n",
        "fe_cb = FeatureExtraction(rmv_stopword=False)\n",
        "fe_ed = FeatureExtraction(rmv_stopword=True)\n",
        "\n",
        "x_train_ed, x_test_ed = fe_ed.get_features(X_train_ed, X_test_ed)\n",
        "x_train_cb, x_test_cb = fe_cb.get_features(X_train_cb, X_test_cb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4JEJ8PxknV3",
        "outputId": "040b8e4b-dcac-4892-a5c4-f4278dba1264"
      },
      "source": [
        "x_train_ed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17500, 14646)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rj-H4aqVuB3"
      },
      "source": [
        "# 4. Models\n",
        "1. Support Vector Machine (SVM)\n",
        "2. Logistic Regression\n",
        "3. Random Forest Classifier\n",
        "4. XGBoost Classifier\n",
        "5. Multinomial Naive Bayes\n",
        "6. Decision Tree Classifier\n",
        "7. Multi-Layer Perceptron (MLP)\n",
        "\n",
        "**Performance Matrix :**\n",
        "1.   **Accuracy** = $\\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}}$\n",
        "2.   **Precision** = $\\frac{\\text{TP}}{\\text{TP+FP}}$ \n",
        "3.   **Recall** = $\\frac{\\text{TP}}{\\text{TP+FN}}$ \n",
        "4.   **F1-score** = $\\frac{\\text{2*Precision*Recall}}{\\text{Precision+Recall}}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syXVrXsGVywC"
      },
      "source": [
        "class Models:\n",
        "    def __init__(self, X_train, Y_train, X_test, Y_test, model_name='cb'):\n",
        "        self.x_train = X_train\n",
        "        self.x_test = X_test\n",
        "        self.y_test = Y_test\n",
        "        self.y_train = Y_train\n",
        "        self.chatbot_model_file = root_path+'/Models/Chatbot Models_7 models.pkl'\n",
        "        self.emotion_model_file = root_path+'/Models/Emotion Detection Models_7 models.pkl'\n",
        "\n",
        "        self.chatbot_summary_file = root_path+'/Models/Chatbot Models Summary_7 models.pkl'\n",
        "        self.emotion_summary_file = root_path+'/Models/Emotion Detection Models Summary_7 models.pkl'\n",
        "        self.model_name = model_name    # cb = chatbot model, ed = emotion detection model\n",
        "\n",
        "        self.svm = SGDClassifier()\n",
        "        self.logisticRegr = LogisticRegression()\n",
        "        self.rfc = RandomForestClassifier(n_estimators=1, random_state=0)\n",
        "        self.xgbc = XGBClassifier(max_depth=16, n_estimators=1000,nthread = 6)\n",
        "        self.mnb = MultinomialNB()\n",
        "        self.dt = tree.DecisionTreeClassifier()\n",
        "        self.mlp = MLPClassifier(random_state=5, max_iter=300)\n",
        "\n",
        "        self.svm_summary = {}\n",
        "        self.lr_summary = {}\n",
        "        self.rfc_summary = {}\n",
        "        self.xgbc_summary = {}\n",
        "        self.mnb_summary = {}\n",
        "        self.dt_summary = {}\n",
        "        self.mlp_summary = {}\n",
        "\n",
        "    def load_models(self):\n",
        "        if self.model_name == 'ed':\n",
        "            if os.path.isfile(self.emotion_model_file):\n",
        "                with open(self.emotion_model_file,'rb') as f:\n",
        "                    self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp = pickle.load(f)\n",
        "\n",
        "                with open(self.emotion_summary_file,'rb') as f:\n",
        "                    self.svm_summary, self.lr_summary, self.rfc_summary, self.xgbc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary = pickle.load(f)\n",
        "                    print('Emotion Detection Models retrived from Disk successfully')\n",
        "                    return self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp\n",
        "            else:\n",
        "                self.train_models()\n",
        "                self.save_models()\n",
        "                return self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp\n",
        "        elif self.model_name == 'cb':\n",
        "            if os.path.isfile(self.chatbot_model_file):\n",
        "                with open(self.chatbot_model_file,'rb') as f:\n",
        "                    self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp = pickle.load(f)\n",
        "\n",
        "                with open(self.chatbot_summary_file,'rb') as f:\n",
        "                    self.svm_summary, self.lr_summary, self.rfc_summary, self.xgbc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary = pickle.load(f)\n",
        "                    print('Chabot Models retrived from Disk successfully')\n",
        "                    return self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp\n",
        "            else:\n",
        "                self.train_models()\n",
        "                self.save_models()\n",
        "                return self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp\n",
        "    def train_models(self):\n",
        "        print('-----Model Training-----')\n",
        "        print('Training SVM...')\n",
        "        self.SVM()\n",
        "        print('Training Logistic Regression...')\n",
        "        self.LR()\n",
        "        print('Training Random Forest...')\n",
        "        self.RFC()\n",
        "        print('Training XGBoost...')\n",
        "        self.XGBC()\n",
        "        print('Training Multinomial Naive Bayes...')\n",
        "        self.MNB()\n",
        "        print('Training Decision Tree...')\n",
        "        self.DT()\n",
        "        print('Training Multi-Layer Perceptron Model...')\n",
        "        self.MLP()\n",
        "        print('Successfully Trained All Models')\n",
        "\n",
        "        return self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp\n",
        "          \n",
        "\n",
        "    def SVM(self):\n",
        "        self.svm.fit(self.x_train, self.y_train)\n",
        "        y_pred = self.svm.predict(self.x_test)\n",
        "\n",
        "        svm_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        svm_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        svm_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        svm_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        svm_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.svm_summary['Accuracy'] = svm_acc\n",
        "        self.svm_summary['Precision'] = svm_prec\n",
        "        self.svm_summary['Recall'] = svm_recal\n",
        "        self.svm_summary['F1'] = svm_f1\n",
        "        self.svm_summary['CM'] = svm_cm\n",
        "    \n",
        "    def LR(self):\n",
        "        self.logisticRegr.fit(self.x_train, self.y_train)\n",
        "\n",
        "        y_pred = self.logisticRegr.predict(self.x_test)\n",
        "\n",
        "        lr_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        lr_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        lr_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        lr_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        lr_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.lr_summary['Accuracy'] = lr_acc\n",
        "        self.lr_summary['Precision'] = lr_prec\n",
        "        self.lr_summary['Recall'] = lr_recal\n",
        "        self.lr_summary['F1'] = lr_f1\n",
        "        self.lr_summary['CM'] = lr_cm\n",
        "\n",
        "    def RFC(self):\n",
        "        self.rfc.fit(self.x_train, self.y_train)\n",
        "\n",
        "        y_pred = self.rfc.predict(self.x_test)\n",
        "\n",
        "        rfc_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        rfc_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        rfc_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        rfc_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        rfc_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.rfc_summary['Accuracy'] = rfc_acc\n",
        "        self.rfc_summary['Precision'] = rfc_prec\n",
        "        self.rfc_summary['Recall'] = rfc_recal\n",
        "        self.rfc_summary['F1'] = rfc_f1\n",
        "        self.rfc_summary['CM'] = rfc_cm\n",
        "\n",
        "    def XGBC(self):\n",
        "        self.xgbc.fit(self.x_train,self.y_train)\n",
        "        y_pred = self.xgbc.predict(self.x_test)\n",
        "\n",
        "        xgbc_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        xgbc_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        xgbc_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        xgbc_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        xgbc_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.xgbc_summary['Accuracy'] = xgbc_acc\n",
        "        self.xgbc_summary['Precision'] = xgbc_prec\n",
        "        self.xgbc_summary['Recall'] = xgbc_recal\n",
        "        self.xgbc_summary['F1'] = xgbc_f1\n",
        "        self.xgbc_summary['CM'] = xgbc_cm\n",
        "\n",
        "    def MNB(self):\n",
        "        self.mnb.fit(self.x_train, self.y_train)\n",
        "\n",
        "        y_pred = self.mnb.predict(self.x_test)\n",
        "\n",
        "        mnb_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        mnb_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        mnb_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        mnb_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        mnb_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.mnb_summary['Accuracy'] = mnb_acc\n",
        "        self.mnb_summary['Precision'] = mnb_prec\n",
        "        self.mnb_summary['Recall'] = mnb_recal\n",
        "        self.mnb_summary['F1'] = mnb_f1\n",
        "        self.mnb_summary['CM'] = mnb_cm\n",
        "\n",
        "    def DT(self):\n",
        "        self.dt.fit(self.x_train, self.y_train)\n",
        "        y_pred = self.dt.predict(self.x_test)\n",
        "\n",
        "        dt_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        dt_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        dt_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        dt_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        dt_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.dt_summary['Accuracy'] = dt_acc\n",
        "        self.dt_summary['Precision'] = dt_prec\n",
        "        self.dt_summary['Recall'] = dt_recal\n",
        "        self.dt_summary['F1'] = dt_f1\n",
        "        self.dt_summary['CM'] = dt_cm\n",
        "\n",
        "    def MLP(self):\n",
        "        self.mlp.fit(self.x_train, self.y_train)\n",
        "        y_pred = self.mlp.predict(self.x_test)\n",
        "\n",
        "        mlp_acc = round(accuracy_score(y_pred, self.y_test)*100,3)\n",
        "        mlp_prec = round(precision_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        mlp_recal = round(recall_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        mlp_cm = confusion_matrix(self.y_test,y_pred)\n",
        "        mlp_f1 = round(f1_score(self.y_test, y_pred, average='macro')*100,3)\n",
        "        self.mlp_summary['Accuracy'] = mlp_acc\n",
        "        self.mlp_summary['Precision'] = mlp_prec\n",
        "        self.mlp_summary['Recall'] = mlp_recal\n",
        "        self.mlp_summary['F1'] = mlp_f1\n",
        "        self.mlp_summary['CM'] = mlp_cm\n",
        "\n",
        "    def model_summary(self):\n",
        "        return self.svm_summary, self.lr_summary, self.rfc_summary, self.xgbc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary\n",
        "\n",
        "    def save_models(self):\n",
        "      if self.model_name == 'ed':\n",
        "          with open(self.emotion_model_file, 'wb') as f:\n",
        "              pickle.dump([self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp], f)\n",
        "\n",
        "          with open(self.emotion_summary_file, 'wb') as f:\n",
        "              pickle.dump([self.svm_summary, self.lr_summary, self.rfc_summary, self.xgbc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary], f)\n",
        "\n",
        "          print('Emotion Detection Models saved successfully in the disk')\n",
        "      elif self.model_name == 'cb':\n",
        "          with open(self.chatbot_model_file, 'wb') as f:\n",
        "              pickle.dump([self.svm, self.logisticRegr, self.rfc, self.xgbc, self.mnb, self.dt, self.mlp], f)\n",
        "\n",
        "          with open(self.chatbot_summary_file, 'wb') as f:\n",
        "              pickle.dump([self.svm_summary, self.lr_summary, self.rfc_summary, self.xgbc_summary, self.mnb_summary, self.dt_summary, self.mlp_summary], f)\n",
        "\n",
        "          print('Chatbot Models saved successfully in the disk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjN-CPxiurd-"
      },
      "source": [
        "chatbot_models = Models(x_train_cb, y_train_cb, x_test_cb, y_test_cb, model_name='cb')\n",
        "emotion_models = Models(x_train_ed, y_train_ed, x_test_ed, y_test_ed, model_name='ed')\n",
        "\n",
        "# svm_cb, logisticRegr_cb, rfc_cb, xgbc_cb, mnb_cb, dt_cb = chatbot_models.train_models()\n",
        "# svm_summary_cb, lr_summary_cb, rfc_summary_cb, xgbc_summary_cb, mnb_summary_cb, dt_summary_cb = chatbot_models.model_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the models"
      ],
      "metadata": {
        "id": "CBbbn3lFd-Xz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd3eQPGkykuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5444d06-eae1-4950-ce62-2bc4839b0bf2"
      },
      "source": [
        "svm_cb, logisticRegr_cb, rfc_cb, xgbc_cb, mnb_cb, dt_cb, mlp_cb = chatbot_models.load_models()\n",
        "svm_summary_cb, lr_summary_cb, rfc_summary_cb, xgbc_summary_cb, mnb_summary_cb, dt_summary_cb, mlp_summary_cb = chatbot_models.model_summary()\n",
        "\n",
        "svm_ed, logisticRegr_ed, rfc_ed, xgbc_ed, mnb_ed, dt_ed, mlp_ed = emotion_models.load_models()\n",
        "svm_summary_ed, lr_summary_ed, rfc_summary_ed, xgbc_summary_ed, mnb_summary_ed, dt_summary_ed, mlp_summary_ed = emotion_models.model_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chabot Models retrived from Disk successfully\n",
            "Emotion Detection Models retrived from Disk successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the models"
      ],
      "metadata": {
        "id": "NQDeZ-omd44N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czqB-sBo-Kos",
        "outputId": "d4e3f613-184c-4700-e988-565523886c7b"
      },
      "source": [
        "svm_cb, logisticRegr_cb, rfc_cb, xgbc_cb, mnb_cb, dt_cb, mlp_cb = chatbot_models.train_models()\n",
        "svm_summary_cb, lr_summary_cb, rfc_summary_cb, xgbc_summary_cb, mnb_summary_cb, dt_summary_cb, mlp_summary_cb = chatbot_models.model_summary()\n",
        "chatbot_models.save_models()\n",
        "\n",
        "svm_ed, logisticRegr_ed, rfc_ed, xgbc_ed, mnb_ed, dt_ed, mlp_ed = emotion_models.train_models()\n",
        "svm_summary_ed, lr_summary_ed, rfc_summary_ed, xgbc_summary_ed, mnb_summary_ed, dt_summary_ed, mlp_summary_ed = emotion_models.model_summary()\n",
        "emotion_models.save_models()\n",
        "\n",
        "print('Accuracy of Emotion Detection Model')\n",
        "print('SVM:',svm_summary_ed['Accuracy'])\n",
        "print('Logistic Regression:',lr_summary_ed['Accuracy'])\n",
        "print('Random Forest:',rfc_summary_ed['Accuracy'])\n",
        "print('XGBoost:',xgbc_summary_ed['Accuracy'])\n",
        "print('Naive Bayes:',mnb_summary_ed['Accuracy'])\n",
        "print('Decision Tree:',dt_summary_ed['Accuracy'])\n",
        "print('MLP:',mlp_summary_ed['Accuracy'])\n",
        "\n",
        "print('\\n\\nAccuracy of Chatbot Model')\n",
        "print('SVM:',svm_summary_cb['Accuracy'])\n",
        "print('Logistic Regression:',lr_summary_cb['Accuracy'])\n",
        "print('Random Forest:',rfc_summary_cb['Accuracy'])\n",
        "print('XGBoost:',xgbc_summary_cb['Accuracy'])\n",
        "print('Naive Bayes:',mnb_summary_cb['Accuracy'])\n",
        "print('Decision Tree:',dt_summary_cb['Accuracy'])\n",
        "print('MLP:',mlp_summary_cb['Accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Model Training-----\n",
            "Training SVM...\n",
            "Training Logistic Regression...\n",
            "Training Random Forest...\n",
            "Training XGBoost...\n",
            "Training Multinomial Naive Bayes...\n",
            "Training Decision Tree...\n",
            "Training Multi-Layer Perceptron Model...\n",
            "Successfully Trained All Models\n",
            "Chatbot Models saved successfully in the disk\n",
            "-----Model Training-----\n",
            "Training SVM...\n",
            "Training Logistic Regression...\n",
            "Training Random Forest...\n",
            "Training XGBoost...\n",
            "Training Multinomial Naive Bayes...\n",
            "Training Decision Tree...\n",
            "Training Multi-Layer Perceptron Model...\n",
            "Successfully Trained All Models\n",
            "Emotion Detection Models saved successfully in the disk\n",
            "Accuracy of Emotion Detection Model\n",
            "SVM: 88.653\n",
            "Logistic Regression: 86.12\n",
            "Random Forest: 66.867\n",
            "XGBoost: 88.84\n",
            "Naive Bayes: 71.587\n",
            "Decision Tree: 76.107\n",
            "MLP: 85.307\n",
            "\n",
            "\n",
            "Accuracy of Chatbot Model\n",
            "SVM: 70.543\n",
            "Logistic Regression: 53.876\n",
            "Random Forest: 44.186\n",
            "XGBoost: 51.744\n",
            "Naive Bayes: 41.085\n",
            "Decision Tree: 49.806\n",
            "MLP: 70.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6H1oXQRAGeB",
        "outputId": "e761e5fe-9960-499b-ca03-b36873468299"
      },
      "source": [
        "print('Accuracy')\n",
        "print('SVM:',svm_summary_ed['Accuracy'])\n",
        "print('Logistic Regression:',lr_summary_ed['Accuracy'])\n",
        "print('Random Forest:',rfc_summary_ed['Accuracy'])\n",
        "print('XGBoost:',xgbc_summary_ed['Accuracy'])\n",
        "print('Naive Bayes:',mnb_summary_ed['Accuracy'])\n",
        "print('Decision Tree:',dt_summary_ed['Accuracy'])\n",
        "print('Decision Tree:',mlp_summary_ed['Accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy\n",
            "SVM: 88.533\n",
            "Logistic Regression: 86.12\n",
            "Random Forest: 66.867\n",
            "XGBoost: 88.84\n",
            "Naive Bayes: 71.587\n",
            "Decision Tree: 75.973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuGzMxw1XSqZ"
      },
      "source": [
        "# 5. Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9xU8nnMaDjY"
      },
      "source": [
        "class Chatbot:\n",
        "    def __init__(self):\n",
        "        accuracies = np.array([svm_summary_cb['Accuracy'], lr_summary_cb['Accuracy'], rfc_summary_cb['Accuracy'], xgbc_summary_cb['Accuracy'], \n",
        "             mnb_summary_cb['Accuracy'], dt_summary_cb['Accuracy'], mlp_summary_cb['Accuracy']])\n",
        "        norm_accuracy = accuracies - min(accuracies)\n",
        "        self.model_weight = norm_accuracy/sum(norm_accuracy)\n",
        "        self.Intents = df_chatbot['Intent'].unique()\n",
        "        self.Human_name = 'Hridoy'\n",
        "\n",
        "    def response_generate(self, text, intent_name):\n",
        "        reply = self.respond(text, intent_name)\n",
        "        return reply\n",
        "\n",
        "    def cosine_distance_countvectorizer_method(self, s1, s2):    \n",
        "        # sentences to list\n",
        "        allsentences = [s1 , s2]\n",
        "\n",
        "        # text to vector\n",
        "        vectorizer = CountVectorizer()\n",
        "        all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
        "\n",
        "        text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
        "        text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
        "\n",
        "        # distance of similarity\n",
        "        cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
        "        return round((1-cosine),2)\n",
        "\n",
        "    def respond(self, text, intent_name):\n",
        "        maximum = float('-inf')\n",
        "        response = \"\"\n",
        "        closest = \"\"\n",
        "        replies = {}\n",
        "        list_sim, list_replies = [],[]\n",
        "        dataset = df_chatbot[df_chatbot['Intent']==intent_name]\n",
        "        for i in dataset.iterrows():\n",
        "            sim = self.cosine_distance_countvectorizer_method(text, i[1]['User'])\n",
        "            list_sim.append(sim)\n",
        "            list_replies.append(i[1]['Chatbot'])\n",
        "\n",
        "        for i in range(len(list_sim)):\n",
        "            if list_sim[i] in replies:\n",
        "                replies[list_sim[i]].append(list_replies[i])\n",
        "            else:\n",
        "                replies[list_sim[i]] = list()\n",
        "                replies[list_sim[i]].append(list_replies[i])\n",
        "        d1 = sorted(replies.items(), key = lambda pair:pair[0],reverse=True)\n",
        "        return d1[0][1][random.randint(0,len(d1[0][1])-1)]\n",
        "\n",
        "\n",
        "    def extract_best_intent(self, list_intent_pred):\n",
        "        intent_scores = {}\n",
        "        for intent in self.Intents:\n",
        "            intent_scores[intent] = 0.0   \n",
        "        for i in range(len(list_intent_pred)):\n",
        "            intent_scores[list_intent_pred[i]] += self.model_weight[i]\n",
        "        si = sorted(intent_scores.items(), key = lambda pair:pair[1],reverse=True)[:6]\n",
        "        return si[0][0], round(si[0][1],2)\n",
        "\n",
        "    def get_human_names(self, text):\n",
        "        person_list = []\n",
        "        person_names=person_list\n",
        "        tokens = nltk.tokenize.word_tokenize(text)\n",
        "        pos = nltk.pos_tag(tokens)\n",
        "        sentt = nltk.ne_chunk(pos, binary = False)\n",
        "\n",
        "        person = []\n",
        "        name = \"\"\n",
        "        for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
        "            for leaf in subtree.leaves():\n",
        "                person.append(leaf[0])\n",
        "            if len(person) > 0: #avoid grabbing lone surnames\n",
        "                for part in person:\n",
        "                    name += part + ' '\n",
        "                if name[:-1] not in person_list:\n",
        "                    person_list.append(name[:-1])\n",
        "                name = ''\n",
        "            person = []\n",
        "        # print (person_list)\n",
        "        return person_list\n",
        "\n",
        "    def replace_tag(self, text):\n",
        "        text = text.replace('<HUMAN>',self.Human_name)\n",
        "\n",
        "        # get current time\n",
        "        BDT = pendulum.timezone('Asia/Dhaka')\n",
        "        cdt = datetime.timetuple(datetime.now(BDT))\n",
        "        hrs = int(cdt[3])\n",
        "        am_pm = 'am'\n",
        "        if int(cdt[3]) > 12:\n",
        "            hrs = int(cdt[3]) - 12\n",
        "            am_pm = 'pm'\n",
        "\n",
        "        current_time = str(cdt[2])+'-'+str(cdt[1])+'-'+str(cdt[0]) + ' '+ str(hrs)+':'+str(cdt[4])+' '+am_pm\n",
        "        text = text.replace('<TIME>',current_time)\n",
        "        return text\n",
        "\n",
        "    def chatbot_reply(self, text):\n",
        "        processed_text = fe_cb.get_processed_text(text)\n",
        "\n",
        "        if self.get_human_names(text):\n",
        "            self.Human_name = self.get_human_names(text)[0]\n",
        "\n",
        "        print('Intent using SVM: ',end = '')\n",
        "        svm_intent = svm_cb.predict(processed_text)[0]\n",
        "        lr_intent = logisticRegr_cb.predict(processed_text)[0]\n",
        "        dt_intent = dt_cb.predict(processed_text)[0]\n",
        "        mnb_intent = mnb_cb.predict(processed_text)[0]\n",
        "        xgbc_intent = xgbc_cb.predict(processed_text)[0]\n",
        "        rfc_intent = rfc_cb.predict(processed_text)[0]\n",
        "        mlp_intent = mlp_cb.predict(processed_text)[0]\n",
        "        print(svm_intent)\n",
        "        \n",
        "        print('Intent using Logistic Regression: ',end = '')\n",
        "        print(lr_intent)\n",
        "        print('Intent using Decision Tree: ',end = '')\n",
        "        print(dt_intent)\n",
        "        print('Intent using Naive Bayes: ',end = '')\n",
        "        print(mnb_intent)\n",
        "        print('Intent using XGBoost: ',end = '')\n",
        "        print(xgbc_intent)\n",
        "        print('Intent using Random Forest: ',end = '')\n",
        "        print(rfc_intent)\n",
        "        print('Intent using Multi-Layer Perceptron: ',end = '')\n",
        "        print(mlp_intent)\n",
        "\n",
        "\n",
        "        # generating reply\n",
        "        list_intent = [svm_intent, lr_intent, rfc_intent, xgbc_intent, mnb_intent, dt_intent, mlp_intent]\n",
        "        best_intent, prob = self.extract_best_intent(list_intent)\n",
        "        print('Best Intent:',best_intent,':',prob)\n",
        "\n",
        "        reply = \"I don't understand. Please be specific\" if prob < 0.4 else self.response_generate(text, best_intent)\n",
        "\n",
        "        reply = self.replace_tag(reply)\n",
        "        print('EDAIC:',reply)\n",
        "        print()\n",
        "        return reply, prob, best_intent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcA4moDWdn7x"
      },
      "source": [
        "class Emotion:\n",
        "    def __init__(self):\n",
        "        self.Emotions = df_emotion['sentiment'].unique()\n",
        "        accuracies = np.array([svm_summary_ed['Accuracy'], lr_summary_ed['Accuracy'], rfc_summary_ed['Accuracy'], xgbc_summary_ed['Accuracy'], \n",
        "             mnb_summary_ed['Accuracy'], dt_summary_ed['Accuracy'], mlp_summary_ed['Accuracy']])\n",
        "        norm_accuracy = accuracies - min(accuracies)\n",
        "        self.emotion_model_weight = norm_accuracy/sum(norm_accuracy)\n",
        "\n",
        "    def extract_best_emotion(self, list_emotion_pred):\n",
        "        emotion_scores = {}\n",
        "        for emotions in self.Emotions:\n",
        "            emotion_scores[emotions] = 0.0   \n",
        "        for i in range(len(list_emotion_pred)):\n",
        "            emotion_scores[list_emotion_pred[i]] += self.emotion_model_weight[i]\n",
        "        se = sorted(emotion_scores.items(), key = lambda pair:pair[1],reverse=True)\n",
        "        return se[0][0], round(se[0][1],2)\n",
        "\n",
        "    def detect_emotion(self, text):\n",
        "        processed_text = fe_ed.get_processed_text(text)\n",
        "\n",
        "        svm_emotion = svm_ed.predict(processed_text)[0]\n",
        "        lr_emotion = logisticRegr_ed.predict(processed_text)[0]\n",
        "        dt_emotion = dt_ed.predict(processed_text)[0]\n",
        "        mnb_emotion = mnb_ed.predict(processed_text)[0]\n",
        "        xgbc_emotion = xgbc_ed.predict(processed_text)[0]\n",
        "        rfc_emotion = rfc_ed.predict(processed_text)[0]\n",
        "        mlp_emotion = mlp_ed.predict(processed_text)[0]\n",
        "\n",
        "        list_emotion_pred = [svm_emotion, lr_emotion, rfc_emotion, xgbc_emotion, mnb_emotion, dt_emotion, mlp_emotion]\n",
        "        best_emotion, prob = self.extract_best_emotion(list_emotion_pred)\n",
        "        print('Best Emotion:',best_emotion,':',prob)\n",
        "\n",
        "        print('Emotion using SVM: ',end = '')\n",
        "        print(svm_emotion)\n",
        "        print('Emotion using Logistic Regression: ',end = '')\n",
        "        print(lr_emotion)\n",
        "        print('Emotion using Decision Tree: ',end = '')\n",
        "        print(dt_emotion)\n",
        "        print('Emotion using Naive Bayes: ',end = '')\n",
        "        print(mnb_emotion)\n",
        "        print('Emotion using XGBoost: ',end = '')\n",
        "        print(xgbc_emotion)\n",
        "        print('Emotion using Random Forest: ',end = '')\n",
        "        print(rfc_emotion)\n",
        "        print('Emotion using Multi-Layer Perceptron ',end = '')\n",
        "        print(mlp_emotion)\n",
        "        print()\n",
        "        return best_emotion, prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAVSuu-o3wkM"
      },
      "source": [
        "chatbot = Chatbot()\n",
        "emotion = Emotion()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAs_UyEZ_jlU",
        "outputId": "32d0e5a4-1d98-4ad5-8edc-af9a197ac2a1"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ7fjGYfZBBy",
        "outputId": "8a051887-c49f-4be0-ef8b-d99fbb78d496"
      },
      "source": [
        "msg = \"I am very much excited about my birthday\"\n",
        "\n",
        "_, _, _ = chatbot.chatbot_reply(msg)\n",
        "\n",
        "_, _ = emotion.detect_emotion(msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent using SVM: Info\n",
            "Intent using Logistic Regression: Sad\n",
            "Intent using Decision Tree: Angry_Frustrated\n",
            "Intent using Naive Bayes: Sad\n",
            "Intent using XGBoost: Farewell\n",
            "Intent using Random Forest: Farewell\n",
            "Intent using Multi-Layer Perceptron: Info\n",
            "Best Intent: Info : 0.63\n",
            "EDAIC: What are you going to do?\n",
            "\n",
            "Best Emotion:  Neutral : 0.95\n",
            "Emotion using SVM:  Neutral\n",
            "Emotion using Logistic Regression:  Neutral\n",
            "Emotion using Decision Tree:  Neutral\n",
            "Emotion using Naive Bayes: sadness\n",
            "Emotion using XGBoost:  Neutral\n",
            "Emotion using Random Forest:  Neutral\n",
            "Emotion using Multi-Layer Perceptron  Neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining Model Weights using the basics of Ensemble Learning"
      ],
      "metadata": {
        "id": "KrDDCEBC5bAZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75FsczjnKAir",
        "outputId": "d49184a5-b3d4-4611-ca14-df2c1bac446b"
      },
      "source": [
        "accuracies = np.array([svm_summary_cb['Accuracy'], lr_summary_cb['Accuracy'], rfc_summary_cb['Accuracy'], xgbc_summary_cb['Accuracy'], \n",
        "             mnb_summary_cb['Accuracy'], dt_summary_cb['Accuracy'], mlp_summary_cb['Accuracy']])\n",
        "norm_accuracy = accuracies - min(accuracies)\n",
        "chatbot_model_weight = norm_accuracy/sum(norm_accuracy)\n",
        "\n",
        "print('SVM:',svm_summary_cb['Accuracy'])\n",
        "print('Logistic regression:',lr_summary_cb['Accuracy'])\n",
        "print('Random forest:',rfc_summary_cb['Accuracy'])\n",
        "print('XGBoost:',xgbc_summary_cb['Accuracy'])\n",
        "print('Naive Bayes:',mnb_summary_cb['Accuracy'])\n",
        "print('Decision Tree:',dt_summary_cb['Accuracy'])\n",
        "print('MLP:',mlp_summary_cb['Accuracy'])\n",
        "chatbot_model_weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: 59.167\n",
            "Logistic regression: 44.167\n",
            "Random forest: 30.833\n",
            "XGBoost: 35.0\n",
            "Naive Bayes: 24.167\n",
            "Decision Tree: 38.333\n",
            "MLP: 61.667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.28188298, 0.16107599, 0.05368663, 0.08724681, 0.        ,\n",
              "       0.11409012, 0.30201748])"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq90NxajUs-8",
        "outputId": "12cdcdf4-deea-4a6e-eb3a-5bc9eba9649f"
      },
      "source": [
        "accuracies = np.array([svm_summary_ed['Accuracy'], lr_summary_ed['Accuracy'], rfc_summary_ed['Accuracy'], xgbc_summary_ed['Accuracy'], \n",
        "             mnb_summary_ed['Accuracy'], dt_summary_ed['Accuracy'], mlp_summary_ed['Accuracy']])\n",
        "norm_accuracy = accuracies - min(accuracies)\n",
        "chatbot_model_weight = norm_accuracy/sum(norm_accuracy)\n",
        "\n",
        "\n",
        "print('SVM:',svm_summary_ed['Accuracy'])\n",
        "print('Logistic regression:',lr_summary_ed['Accuracy'])\n",
        "print('Random forest:',rfc_summary_ed['Accuracy'])\n",
        "print('XGBoost:',xgbc_summary_ed['Accuracy'])\n",
        "print('Naive Bayes:',mnb_summary_ed['Accuracy'])\n",
        "print('Decision Tree:',dt_summary_ed['Accuracy'])\n",
        "print('MLP:',mlp_summary_ed['Accuracy'])\n",
        "chatbot_model_weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: 88.52\n",
            "Logistic regression: 86.12\n",
            "Random forest: 66.867\n",
            "XGBoost: 88.84\n",
            "Naive Bayes: 71.587\n",
            "Decision Tree: 75.893\n",
            "MLP: 85.307\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.22777047, 0.20252459, 0.        , 0.23113659, 0.04965024,\n",
              "       0.09494556, 0.19397255])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    }
  ]
}